{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_feather(\"./data/train_feather\")\n",
    "# val = pd.read_feather(\"./data/val_feather\")\n",
    "# test = pd.read_feather(\"./data/test_feather\")\n",
    "# ids = pd.read_feather(\"./data/test_ids\")\n",
    "\n",
    "# train = shuffle(train)\n",
    "# val = shuffle(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "store_nbr      float32\n",
      "item_nbr       float32\n",
      "onpromotion    float32\n",
      "year           float32\n",
      "month          float32\n",
      "dayofmonth     float32\n",
      "dayofweek      float32\n",
      "unit_sales     float32\n",
      "dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>dayofmonth</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>unit_sales</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>38.0</td>\n",
       "      <td>456875.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>40.0</td>\n",
       "      <td>1968666.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>33.0</td>\n",
       "      <td>1109326.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>45.0</td>\n",
       "      <td>1239914.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>38.0</td>\n",
       "      <td>1239897.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7.596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   store_nbr   item_nbr  onpromotion    year  month  dayofmonth  dayofweek  \\\n",
       "0       38.0   456875.0          0.0  2013.0    8.0        13.0        1.0   \n",
       "1       40.0  1968666.0          0.0  2016.0   10.0        13.0        3.0   \n",
       "2       33.0  1109326.0          0.0  2013.0   12.0        11.0        2.0   \n",
       "3       45.0  1239914.0          0.0  2015.0    1.0        17.0        5.0   \n",
       "4       38.0  1239897.0          0.0  2014.0    4.0        20.0        6.0   \n",
       "\n",
       "   unit_sales  \n",
       "0       2.000  \n",
       "1       2.000  \n",
       "2      20.000  \n",
       "3      20.000  \n",
       "4       7.596  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train[['store_nbr', 'item_nbr', 'onpromotion', 'year', 'month', 'dayofmonth', 'dayofweek', 'unit_sales']]\n",
    "print(train.dtypes)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.columns:\n",
    "    train[col] = train[col].astype(np.float32)\n",
    "for col in val.columns:\n",
    "    val[col] = val[col].astype(np.float32)\n",
    "for col in test.columns:\n",
    "    test[col] = test[col].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.reset_index(inplace=True)\n",
    "# val.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_feather(\"./data/train_feather\")\n",
    "# val.to_feather(\"./data/val_feather\")\n",
    "# test.to_feather(\"./data/test_feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float32'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 300\n",
    "batch_size = 256\n",
    "display_step = 5\n",
    "\n",
    "# Indices to traverse dataset\n",
    "train_ptr = 0\n",
    "val_ptr = 0\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128 # 1st layer number of neurons\n",
    "n_hidden_2 = 128 # 2nd layer number of neurons\n",
    "n_hidden_3 = 128 # 2nd layer number of neurons\n",
    "num_input = 4207 # one-hot encoding of featuers\n",
    "num_classes = 1 # regression\n",
    "\n",
    "# Sizes of datasets\n",
    "num_samples = train.shape[0]\n",
    "num_samples_val = val.shape[0]\n",
    "num_samples_test = test.shape[0]\n",
    "\n",
    "# For one-hot encoding of the features\n",
    "encoding_vals = np.load(\"./data/encoding_vals.npy\")\n",
    "enc = OneHotEncoder(dtype=np.float32)\n",
    "enc.fit(encoding_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traverses dataset using begin/end\n",
    "# One-hot encodes all the features as they are all categorical\n",
    "def get_batch_train(begin, end):\n",
    "    batch = train.iloc[begin:end,:].values\n",
    "    labels = batch[:,-1]\n",
    "    features = batch[:,:-1]\n",
    "    one_hots = enc.transform(features).toarray()\n",
    "\n",
    "    return (tf.convert_to_tensor(one_hots), tf.convert_to_tensor(labels))\n",
    "\n",
    "def get_batch_val(begin, end):\n",
    "    batch = val.iloc[begin:end,:].values\n",
    "    labels = batch[:,-1]\n",
    "    features = batch[:,:-1]\n",
    "    one_hots = enc.transform(features).toarray()\n",
    "\n",
    "    return (tf.convert_to_tensor(one_hots), tf.convert_to_tensor(labels))\n",
    "\n",
    "def get_batch_test(begin, end):\n",
    "    batch = test.iloc[begin:end,:].values\n",
    "    one_hots = enc.transform(batch).toarray()\n",
    "\n",
    "    return tf.convert_to_tensor(one_hots)\n",
    "\n",
    "def variable_summaries(var):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "\n",
    "# Store layers weight & bias\n",
    "# h10 = tf.get_variable('h10', [num_input, n_hidden_1], initalizer=tf.random_normal_initializer)\n",
    "with tf.name_scope('weights'):\n",
    "    weights = {\n",
    "        'h1': tf.get_variable('h1', [num_input, n_hidden_1], tf.float32, tf.random_normal_initializer()),\n",
    "        'h2': tf.get_variable('h2', [n_hidden_1, n_hidden_2], tf.float32, tf.random_normal_initializer()),\n",
    "        'h3': tf.get_variable('h3', [n_hidden_2, n_hidden_3], tf.float32, tf.random_normal_initializer()),\n",
    "        'out': tf.get_variable('out', [n_hidden_3, num_classes], tf.float32, tf.random_normal_initializer())  \n",
    "    }\n",
    "    \n",
    "with tf.name_scope('biases'):\n",
    "    biases = {\n",
    "        'b1': tf.get_variable('b1', [n_hidden_1], tf.float32, tf.random_normal_initializer()),\n",
    "        'b2': tf.get_variable('b2', [n_hidden_2], tf.float32, tf.random_normal_initializer()),\n",
    "        'b3': tf.get_variable('b3', [n_hidden_3], tf.float32, tf.random_normal_initializer()),\n",
    "        'outb': tf.get_variable('outb', [num_classes], tf.float32, tf.random_normal_initializer())\n",
    "    }\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# should make a loop\n",
    "# variable_summaries(weights['h1'])\n",
    "# variable_summaries(weights['h2'])\n",
    "# variable_summaries(weights['h3'])\n",
    "# variable_summaries(weights['out'])\n",
    "# variable_summaries(biases['b1'])\n",
    "# variable_summaries(biases['b2'])\n",
    "# variable_summaries(biases['b3'])\n",
    "# variable_summaries(biases['outb'])\n",
    "\n",
    "# Create model\n",
    "with tf.name_scope('nn'):\n",
    "    def neural_net(x):\n",
    "        # Hidden fully connected layer with 128 neurons\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "        tf.summary.histogram(\"L1pre-activations\", layer_1)\n",
    "        l1_act = tf.nn.relu(layer_1)\n",
    "        tf.summary.histogram(\"L1activated\", l1_act)\n",
    "        # Hidden fully connected layer with 128 neurons\n",
    "        layer_2 = tf.add(tf.matmul(l1_act, weights['h2']), biases['b2'])\n",
    "        tf.summary.histogram(\"L2pre-activations\", layer_2)\n",
    "        l2_act = tf.nn.relu(layer_2)\n",
    "        tf.summary.histogram(\"L2activated\", l2_act)\n",
    "        # Hidden fully connected layer with 128 neurons\n",
    "        layer_3 = tf.add(tf.matmul(l2_act, weights['h3']), biases['b3'])\n",
    "        tf.summary.histogram(\"L3pre-activations\", layer_3)\n",
    "        l3_act = tf.nn.relu(layer_3)\n",
    "        tf.summary.histogram(\"L3activated\", l3_act)\n",
    "        # Output fully connected layer with a neuron for prediction\n",
    "        out_layer = tf.matmul(l3_act, weights['out']) + biases['outb']\n",
    "        return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward prop\n",
    "prediction = neural_net(X)\n",
    "tf.summary.histogram(\"output\", prediction)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.losses.mean_squared_error(labels=Y, predictions=prediction)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op, global_step=global_step)\n",
    "\n",
    "# Evaluation\n",
    "mae = tf.metrics.mean_absolute_error(labels=Y, predictions=prediction)\n",
    "\n",
    "# Tensorboard\n",
    "tb_loss = tf.summary.scalar(\"loss\", loss_op)\n",
    "tb_mae = tf.summary.scalar(\"MAE\", tf.reduce_mean(mae))\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring and Intializing\n",
      "INFO:tensorflow:Restoring parameters from ./model_checks/model.ckpt\n",
      "Begin datetime: 2018-01-09 05:21:27.931862\n",
      "Step 1/301, Minibatch Loss = 31872.0, Training MAE = (0.0, 138.83485), Eval MAE = (138.83485, 145.81924)\n",
      "Step 5/305, Minibatch Loss = 31621.5, Training MAE = (145.81924, 144.66667), Eval MAE = (144.66667, 146.38008)\n",
      "Step 10/310, Minibatch Loss = 29482.9, Training MAE = (146.38008, 145.15881), Eval MAE = (145.15881, 148.35052)\n",
      "Step 15/315, Minibatch Loss = 29249.6, Training MAE = (148.35052, 146.76233), Eval MAE = (146.76233, 146.7634)\n",
      "Step 20/320, Minibatch Loss = 30999.7, Training MAE = (146.7634, 146.08112), Eval MAE = (146.08112, 147.23471)\n",
      "Step 25/325, Minibatch Loss = 24157.6, Training MAE = (147.23471, 145.25014), Eval MAE = (145.25014, 145.9659)\n",
      "Step 30/330, Minibatch Loss = 26349.6, Training MAE = (145.9659, 144.1429), Eval MAE = (144.1429, 144.09691)\n",
      "Step 35/335, Minibatch Loss = 23184.8, Training MAE = (144.09691, 142.53951), Eval MAE = (142.53951, 142.18044)\n",
      "Step 40/340, Minibatch Loss = 26790.3, Training MAE = (142.18044, 141.261), Eval MAE = (141.261, 141.02081)\n",
      "Step 45/345, Minibatch Loss = 28173.9, Training MAE = (141.02081, 140.71773), Eval MAE = (140.71773, 140.46667)\n",
      "Step 50/350, Minibatch Loss = 22497.5, Training MAE = (140.46667, 139.48767), Eval MAE = (139.48767, 139.2912)\n",
      "Step 55/355, Minibatch Loss = 26994.4, Training MAE = (139.2912, 138.94347), Eval MAE = (138.94347, 138.59074)\n",
      "Step 60/360, Minibatch Loss = 27519.4, Training MAE = (138.59074, 138.30666), Eval MAE = (138.30666, 138.00009)\n",
      "Step 65/365, Minibatch Loss = 27241.2, Training MAE = (138.00009, 137.49187), Eval MAE = (137.49187, 137.13307)\n",
      "Step 70/370, Minibatch Loss = 27999.9, Training MAE = (137.13307, 137.06941), Eval MAE = (137.06941, 137.03584)\n",
      "Step 75/375, Minibatch Loss = 22446.1, Training MAE = (137.03584, 136.44176), Eval MAE = (136.44176, 136.57539)\n",
      "Step 80/380, Minibatch Loss = 23197.2, Training MAE = (136.57539, 136.04759), Eval MAE = (136.04759, 135.50044)\n",
      "Step 85/385, Minibatch Loss = 20095.2, Training MAE = (135.50044, 134.88152), Eval MAE = (134.88152, 134.48982)\n",
      "Step 90/390, Minibatch Loss = 20723.9, Training MAE = (134.48982, 133.98196), Eval MAE = (133.98196, 133.93562)\n",
      "Step 95/395, Minibatch Loss = 25084.9, Training MAE = (133.93562, 133.68468), Eval MAE = (133.68468, 133.38669)\n",
      "Step 100/400, Minibatch Loss = 21778.9, Training MAE = (133.38669, 132.99728), Eval MAE = (132.99728, 132.78854)\n",
      "Step 105/405, Minibatch Loss = 18156.9, Training MAE = (132.78854, 132.15274), Eval MAE = (132.15274, 132.14824)\n",
      "Step 110/410, Minibatch Loss = 23048.5, Training MAE = (132.14824, 131.79948), Eval MAE = (131.79948, 131.64737)\n",
      "Step 115/415, Minibatch Loss = 18522.5, Training MAE = (131.64737, 131.1546), Eval MAE = (131.1546, 130.84471)\n",
      "Step 120/420, Minibatch Loss = 21317.4, Training MAE = (130.84471, 130.53992), Eval MAE = (130.53992, 130.35754)\n",
      "Step 125/425, Minibatch Loss = 18043.9, Training MAE = (130.35754, 129.84955), Eval MAE = (129.84955, 129.41428)\n",
      "Step 130/430, Minibatch Loss = 19296.2, Training MAE = (129.41428, 129.04935), Eval MAE = (129.04935, 128.78549)\n",
      "Step 135/435, Minibatch Loss = 20664.0, Training MAE = (128.78549, 128.54247), Eval MAE = (128.54247, 128.20178)\n",
      "Step 140/440, Minibatch Loss = 18470.7, Training MAE = (128.20178, 127.76102), Eval MAE = (127.76102, 127.68082)\n",
      "Step 145/445, Minibatch Loss = 19228.6, Training MAE = (127.68082, 127.36609), Eval MAE = (127.36609, 127.1548)\n",
      "Step 150/450, Minibatch Loss = 20151.2, Training MAE = (127.1548, 126.85254), Eval MAE = (126.85254, 126.53388)\n",
      "Step 155/455, Minibatch Loss = 15829.1, Training MAE = (126.53388, 126.08985), Eval MAE = (126.08985, 125.82861)\n",
      "Step 160/460, Minibatch Loss = 17695.0, Training MAE = (125.82861, 125.50993), Eval MAE = (125.50993, 125.36317)\n",
      "Step 165/465, Minibatch Loss = 17467.0, Training MAE = (125.36317, 125.07958), Eval MAE = (125.07958, 124.75726)\n",
      "Step 170/470, Minibatch Loss = 15450.4, Training MAE = (124.75726, 124.37833), Eval MAE = (124.37833, 124.12958)\n",
      "Step 175/475, Minibatch Loss = 12675.6, Training MAE = (124.12958, 123.62301), Eval MAE = (123.62301, 123.47559)\n",
      "Step 180/480, Minibatch Loss = 14665.6, Training MAE = (123.47559, 123.17653), Eval MAE = (123.17653, 122.90993)\n",
      "Step 185/485, Minibatch Loss = 13086.9, Training MAE = (122.90993, 122.45156), Eval MAE = (122.45156, 122.17578)\n",
      "Step 190/490, Minibatch Loss = 16288.0, Training MAE = (122.17578, 121.89688), Eval MAE = (121.89688, 121.72104)\n",
      "Step 195/495, Minibatch Loss = 14887.1, Training MAE = (121.72104, 121.43309), Eval MAE = (121.43309, 121.27354)\n",
      "Step 200/500, Minibatch Loss = 12975.5, Training MAE = (121.27354, 120.90823), Eval MAE = (120.90823, 120.66579)\n",
      "Step 205/505, Minibatch Loss = 13047.7, Training MAE = (120.66579, 120.27129), Eval MAE = (120.27129, 119.98395)\n",
      "Step 210/510, Minibatch Loss = 14902.6, Training MAE = (119.98395, 119.7094), Eval MAE = (119.7094, 119.46907)\n",
      "Step 215/515, Minibatch Loss = 13728.6, Training MAE = (119.46907, 119.14324), Eval MAE = (119.14324, 118.95613)\n",
      "Step 220/520, Minibatch Loss = 17625.7, Training MAE = (118.95613, 118.78658), Eval MAE = (118.78658, 118.63178)\n",
      "Step 225/525, Minibatch Loss = 12621.4, Training MAE = (118.63178, 118.28317), Eval MAE = (118.28317, 118.03556)\n",
      "Step 230/530, Minibatch Loss = 11296.1, Training MAE = (118.03556, 117.66894), Eval MAE = (117.66894, 117.41872)\n",
      "Step 235/535, Minibatch Loss = 14793.0, Training MAE = (117.41872, 117.21455), Eval MAE = (117.21455, 117.09771)\n",
      "Step 240/540, Minibatch Loss = 11487.5, Training MAE = (117.09771, 116.76046), Eval MAE = (116.76046, 116.57128)\n",
      "Step 245/545, Minibatch Loss = 10636.9, Training MAE = (116.57128, 116.20851), Eval MAE = (116.20851, 116.00111)\n",
      "Step 250/550, Minibatch Loss = 12938.2, Training MAE = (116.00111, 115.73962), Eval MAE = (115.73962, 115.48299)\n",
      "Step 255/555, Minibatch Loss = 11833.5, Training MAE = (115.48299, 115.21473), Eval MAE = (115.21473, 115.03153)\n",
      "Step 260/560, Minibatch Loss = 15309.3, Training MAE = (115.03153, 114.77654), Eval MAE = (114.77654, 114.53217)\n",
      "Step 265/565, Minibatch Loss = 10910.8, Training MAE = (114.53217, 114.25045), Eval MAE = (114.25045, 113.91811)\n",
      "Step 270/570, Minibatch Loss = 12665.5, Training MAE = (113.91811, 113.71149), Eval MAE = (113.71149, 113.5614)\n",
      "Step 275/575, Minibatch Loss = 9607.89, Training MAE = (113.5614, 113.21674), Eval MAE = (113.21674, 112.9693)\n",
      "Step 280/580, Minibatch Loss = 11399.2, Training MAE = (112.9693, 112.73531), Eval MAE = (112.73531, 112.58124)\n",
      "Step 285/585, Minibatch Loss = 9526.39, Training MAE = (112.58124, 112.27309), Eval MAE = (112.27309, 112.02701)\n",
      "Step 290/590, Minibatch Loss = 12153.1, Training MAE = (112.02701, 111.8215), Eval MAE = (111.8215, 111.64701)\n",
      "Step 295/595, Minibatch Loss = 9500.88, Training MAE = (111.64701, 111.36978), Eval MAE = (111.36978, 111.14195)\n",
      "Step 300/600, Minibatch Loss = 12679.1, Training MAE = (111.14195, 110.96816), Eval MAE = (110.96816, 110.78533)\n",
      "End datetime: 2018-01-09 05:22:29.389412\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "GraphDef cannot be larger than 2GB.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f772c3bd62f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# Save model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#     print(\"h1 : %s\" % weights['h1'].eval())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0msave_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"./model_checks/model.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model saved at: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, sess, save_path, global_step, latest_filename, meta_graph_suffix, write_meta_graph, write_state)\u001b[0m\n\u001b[1;32m   1494\u001b[0m           checkpoint_file, meta_graph_suffix=meta_graph_suffix)\n\u001b[1;32m   1495\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_meta_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeta_graph_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_empty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mexport_meta_graph\u001b[0;34m(self, filename, collection_list, as_text, export_scope, clear_devices, clear_extraneous_savers)\u001b[0m\n\u001b[1;32m   1526\u001b[0m     return export_meta_graph(\n\u001b[1;32m   1527\u001b[0m         \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1528\u001b[0;31m         \u001b[0mgraph_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madd_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1529\u001b[0m         \u001b[0msaver_def\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaver_def\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[0mcollection_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   2483\u001b[0m     \"\"\"\n\u001b[1;32m   2484\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2485\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_def\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfrom_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2486\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_def\u001b[0;34m(self, from_version, add_shapes)\u001b[0m\n\u001b[1;32m   2444\u001b[0m           \u001b[0mbytesize\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mbytesize\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m<<\u001b[0m \u001b[0;36m31\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mbytesize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"GraphDef cannot be larger than 2GB.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: GraphDef cannot be larger than 2GB."
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    print(\"Restoring and Intializing\")\n",
    "    saver.restore(sess, \"./model_checks/model.ckpt\")\n",
    "#     print(\"h1 : %s\" % weights['h1'].eval())\n",
    "\n",
    "    # Run the initializer\n",
    "#     sess.run(init)\n",
    "    sess.run(init_l)\n",
    "#     print(\"h1 : %s\" % weights['h1'].eval())\n",
    "\n",
    "\n",
    "    # Tensorboard writers\n",
    "    train_writer = tf.summary.FileWriter(\"./log_tb\" + '/train', sess.graph)\n",
    "    test_writer = tf.summary.FileWriter(\"./log_tb\" + '/test')\n",
    "\n",
    "    print(\"Begin datetime: \" + str(datetime.datetime.now()))\n",
    "    # TRAINING ///\n",
    "    for step in range(1, num_steps+1):\n",
    "        try:\n",
    "            # Get batch and use begin/end indices to traverse dataset\n",
    "            batch_x, batch_y = sess.run(get_batch_train(train_ptr, train_ptr+batch_size))\n",
    "            train_ptr+=batch_size\n",
    "            \n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        except IndexError:\n",
    "            print(\"Finished an epoch, shuffling dataset\")\n",
    "            shuffle(train)\n",
    "            train_ptr=0\n",
    "\n",
    "        # VALIDATING and printing stats\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "            summary, loss, mean_abs_train = sess.run([merged, loss_op, mae], feed_dict={X: batch_x, Y: batch_y})\n",
    "            train_writer.add_summary(summary, tf.train.global_step(sess, global_step))\n",
    "\n",
    "            # Validation accuracy\n",
    "            try:\n",
    "                val_batch_x, val_batch_y = sess.run(get_batch_val(val_ptr, val_ptr+batch_size))\n",
    "                val_ptr+=batch_size\n",
    "            except StopIteration:\n",
    "                print(\"Reshuffling validation set\")\n",
    "                shuffle(val)\n",
    "                val_ptr=0\n",
    "\n",
    "                val_batch_x, val_batch_y = sess.run(get_batch_val(val_ptr, val_ptr+batch_size))\n",
    "                val_ptr+=batch_size\n",
    "\n",
    "            summary, mean_abs_val = sess.run([merged, mae], feed_dict={X: val_batch_x, Y: val_batch_y})\n",
    "            test_writer.add_summary(summary, tf.train.global_step(sess, global_step))\n",
    "\n",
    "            print(\"Step \" + str(step) + \"/\" + str(tf.train.global_step(sess, global_step)) + \", Minibatch Loss = \" + \\\n",
    "                  str(loss) + \", Training MAE = \" + \\\n",
    "                  str(mean_abs_train) + \", Eval MAE = \" + str(mean_abs_val))\n",
    "\n",
    "\n",
    "    print(\"End datetime: \" + str(datetime.datetime.now()))\n",
    "    # Save model\n",
    "#     print(\"h1 : %s\" % weights['h1'].eval())\n",
    "    save_path = saver.save(sess, \"./model_checks/model.ckpt\")\n",
    "    print(\"Model saved at: \" + str(save_path))\n",
    "\n",
    "    # Predict values for submission\n",
    "#     submission = np.array([[0,0]])\n",
    "#     counter = 0\n",
    "#     test_ptr = 0\n",
    "    \n",
    "#     while(True):\n",
    "#         if (test_ptr > num_samples_test):\n",
    "#             np.savetxt(\"./submission.txt\", submission, delimiter=',')\n",
    "#             print(\"Saved Submissions!\")\n",
    "#             break\n",
    "            \n",
    "#         try:\n",
    "#             if(counter % 10000 == 0):\n",
    "#                 print(\"batch # \" + str(counter))\n",
    "#             counter+=1\n",
    "\n",
    "#             test_batch = sess.run(get_batch_test(test_ptr, test_ptr+batch_size))\n",
    "#             test_ids = ids.iloc[test_ptr:test_ptr+batch_size].values.reshape((batch_size,-1))\n",
    "#             test_ptr+=batch_size\n",
    "\n",
    "#             predictions = sess.run([prediction], feed_dict={X: test_batch})\n",
    "#             preds = sess.run(tf.squeeze(preds)).reshape((batch_size, -1))\n",
    "#             new_subs = sess.run(tf.squeeze(np.array([test_ids, preds]).T))\n",
    "#             submission = np.concatenate((submission, new_subs))\n",
    "#             print(\"Concatenated!\")\n",
    "\n",
    "#         except IndexError:\n",
    "#             np.savetxt(\"./submission.txt\", submission, delimiter=',')\n",
    "#             print(\"Saved Submissions!\")\n",
    "#             break\n",
    "\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
