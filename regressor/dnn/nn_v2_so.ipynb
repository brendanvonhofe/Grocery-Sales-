{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train = pd.read_feather(\"./data/train_feather\")\n",
    "val = pd.read_feather(\"./data/val_feather\")\n",
    "test = pd.read_feather(\"./data/test_feather\")\n",
    "ids = pd.read_feather(\"./data/test_ids\")\n",
    "\n",
    "# Shuffle time\n",
    "train = shuffle(train)\n",
    "val = shuffle(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float32'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "num_steps = 500\n",
    "batch_size = 512\n",
    "display_step = 50\n",
    "\n",
    "# Indices to traverse dataset\n",
    "train_ptr = 0\n",
    "val_ptr = 0\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 128 # 1st layer number of neurons\n",
    "n_hidden_2 = 128 # 2nd layer number of neurons\n",
    "n_hidden_3 = 128 # 2nd layer number of neurons\n",
    "num_input = 4207 # one-hot encoding of featuers\n",
    "num_classes = 1 # regression\n",
    "\n",
    "# Sizes of datasets\n",
    "num_samples = train.shape[0]\n",
    "num_samples_val = val.shape[0]\n",
    "num_samples_test = test.shape[0]\n",
    "\n",
    "# For one-hot encoding of the features\n",
    "encoding_vals = np.load(\"./data/encoding_vals.npy\")\n",
    "enc = OneHotEncoder(dtype=np.float32)\n",
    "enc.fit(encoding_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traverses dataset using begin/end\n",
    "# One-hot encodes all the features as they are all categorical\n",
    "def get_batch_train(begin, end):\n",
    "    batch = train.iloc[begin:end,:].values\n",
    "    labels = batch[:,-1]\n",
    "    features = batch[:,:-1]\n",
    "    one_hots = enc.transform(features).toarray()\n",
    "#     inputs = tf.convert_to_tensor(one_hots)\n",
    "#     targets = tf.convert_to_tensor(labels)\n",
    "    \n",
    "    return (one_hots, labels)\n",
    "\n",
    "def get_batch_val(begin, end):\n",
    "    batch = val.iloc[begin:end,:].values\n",
    "    item_nums = batch[:,1]\n",
    "    labels = batch[:,-1]\n",
    "    features = batch[:,:-1]\n",
    "    one_hots = enc.transform(features).toarray()\n",
    "#     inputs = tf.convert_to_tensor(one_hots)\n",
    "#     targets = tf.convert_to_tensor(labels)\n",
    "    \n",
    "    return (one_hots, labels)\n",
    "\n",
    "def get_batch_test(begin, end):\n",
    "    batch = test.iloc[begin:end,:].values\n",
    "    one_hots = enc.transform(batch).toarray()\n",
    "#     inputs = tf.convert_to_tensor(one_hots)\n",
    "    \n",
    "    return one_hots\n",
    "\n",
    "def variable_summaries(var):\n",
    "  \"\"\"Attach a lot of summaries to a Tensor (for TensorBoard visualization).\"\"\"\n",
    "  with tf.name_scope('summaries'):\n",
    "    mean = tf.reduce_mean(var)\n",
    "    tf.summary.scalar('mean', mean)\n",
    "    with tf.name_scope('stddev'):\n",
    "      stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "    tf.summary.scalar('stddev', stddev)\n",
    "    tf.summary.scalar('max', tf.reduce_max(var))\n",
    "    tf.summary.scalar('min', tf.reduce_min(var))\n",
    "    tf.summary.histogram('histogram', var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\")\n",
    "Y = tf.placeholder(\"float\")\n",
    "# item = tf.placeholder(\"float\")\n",
    "\n",
    "# Store layers weight & bias\n",
    "# h10 = tf.get_variable('h10', [num_input, n_hidden_1], initalizer=tf.random_normal_initializer)\n",
    "with tf.name_scope('weights'):\n",
    "    weights = {\n",
    "        'h1': tf.get_variable('h1', [num_input, n_hidden_1], tf.float32, tf.random_normal_initializer()),\n",
    "        'h2': tf.get_variable('h2', [n_hidden_1, n_hidden_2], tf.float32, tf.random_normal_initializer()),\n",
    "        'h3': tf.get_variable('h3', [n_hidden_2, n_hidden_3], tf.float32, tf.random_normal_initializer()),\n",
    "        'out': tf.get_variable('out', [n_hidden_3, num_classes], tf.float32, tf.random_normal_initializer())  \n",
    "    }\n",
    "    \n",
    "with tf.name_scope('biases'):\n",
    "    biases = {\n",
    "        'b1': tf.get_variable('b1', [n_hidden_1], tf.float32, tf.random_normal_initializer()),\n",
    "        'b2': tf.get_variable('b2', [n_hidden_2], tf.float32, tf.random_normal_initializer()),\n",
    "        'b3': tf.get_variable('b3', [n_hidden_3], tf.float32, tf.random_normal_initializer()),\n",
    "        'outb': tf.get_variable('outb', [num_classes], tf.float32, tf.random_normal_initializer())\n",
    "    }\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "# should make a loop\n",
    "# variable_summaries(weights['h1'])\n",
    "# variable_summaries(weights['h2'])\n",
    "# variable_summaries(weights['h3'])\n",
    "# variable_summaries(weights['out'])\n",
    "# variable_summaries(biases['b1'])\n",
    "# variable_summaries(biases['b2'])\n",
    "# variable_summaries(biases['b3'])\n",
    "# variable_summaries(biases['outb'])\n",
    "\n",
    "# Create model\n",
    "with tf.name_scope('nn'):\n",
    "    def neural_net(x):\n",
    "        # Hidden fully connected layer with 128 neurons\n",
    "        layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "#         tf.summary.histogram(\"L1pre-activations\", layer_1)\n",
    "        l1_act = tf.nn.relu(layer_1)\n",
    "#         tf.summary.histogram(\"L1activated\", l1_act)\n",
    "        # Hidden fully connected layer with 128 neurons\n",
    "        layer_2 = tf.add(tf.matmul(l1_act, weights['h2']), biases['b2'])\n",
    "#         tf.summary.histogram(\"L2pre-activations\", layer_2)\n",
    "        l2_act = tf.nn.relu(layer_2)\n",
    "#         tf.summary.histogram(\"L2activated\", l2_act)\n",
    "        # Hidden fully connected layer with 128 neurons\n",
    "        layer_3 = tf.add(tf.matmul(l2_act, weights['h3']), biases['b3'])\n",
    "#         tf.summary.histogram(\"L3pre-activations\", layer_3)\n",
    "        l3_act = tf.nn.relu(layer_3)\n",
    "#         tf.summary.histogram(\"L3activated\", l3_act)\n",
    "        # Output fully connected layer with a neuron for prediction\n",
    "        out_layer = tf.matmul(l3_act, weights['out']) + biases['outb']\n",
    "        return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward prop\n",
    "prediction = neural_net(X)\n",
    "# tf.summary.histogram(\"output\", prediction)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.losses.mean_squared_error(labels=Y, predictions=prediction)\n",
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op, global_step=global_step)\n",
    "\n",
    "# w = [1.25 if (items.perishable.iloc[np.where(items['item_nbr']==x)[0][0]]) else 1 for x in item.eval(session=sess)]\n",
    "# w = np.asarray(w)\n",
    "# nwrmsle = tf.sqrt(tf.reduce_sum(tf.multiply(tf.square(tf.log(tf.add(predictions), tf.constant(1)) - tf.log(tf.add(Y, tf.constant(1))), w)))/tf.reduce_sum(w))\n",
    "# Evaluation\n",
    "mae = tf.metrics.mean_absolute_error(labels=Y, predictions=prediction)\n",
    "\n",
    "# Tensorboard\n",
    "# tb_loss = tf.summary.scalar(\"loss\", loss_op)\n",
    "# tb_mae = tf.summary.scalar(\"MAE\", tf.reduce_mean(mae))\n",
    "# merged = tf.summary.merge_all()\n",
    "\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init_global = tf.global_variables_initializer()\n",
    "init_local = tf.local_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring and Intializing\n",
      "INFO:tensorflow:Restoring parameters from ./model_checks/model.ckpt\n",
      "Begin datetime: 2018-01-09 18:20:41.964151\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Fetch argument array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n       ..., \n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    269\u001b[0m         self._unique_fetches.append(ops.get_default_graph().as_graph_element(\n\u001b[0;32m--> 270\u001b[0;31m             fetch, allow_tensor=True, allow_operation=True))\n\u001b[0m\u001b[1;32m    271\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mas_graph_element\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2707\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2708\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_as_graph_element_locked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_operation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_as_graph_element_locked\u001b[0;34m(self, obj, allow_tensor, allow_operation)\u001b[0m\n\u001b[1;32m   2796\u001b[0m       raise TypeError(\"Can not convert a %s into a %s.\"\n\u001b[0;32m-> 2797\u001b[0;31m                       % (type(obj).__name__, types_str))\n\u001b[0m\u001b[1;32m   2798\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Can not convert a ndarray into a Tensor or Operation.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-f12fd44ff47e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Get batch and use begin/end indices to traverse dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_batch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_ptr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ptr\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mtrain_ptr\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1107\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m     fetch_handler = _FetchHandler(\n\u001b[0;32m-> 1109\u001b[0;31m         self._graph, fetches, feed_dict_tensor, feed_handles=feed_handles)\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;31m# Run request and get response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, graph, fetches, feeds, feed_handles)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \"\"\"\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_mapper\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m       \u001b[0;31m# NOTE(touts): This is also the code path for namedtuples.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_ListFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_DictFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \"\"\"\n\u001b[1;32m    339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \"\"\"\n\u001b[1;32m    339\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetch_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_FetchMapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfor_fetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfetch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unique_fetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_uniquify_fetches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mappers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mfor_fetch\u001b[0;34m(fetch)\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m           \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 241\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0m_ElementFetchMapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontraction_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m     \u001b[0;31m# Did not find anything.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     raise TypeError('Fetch argument %r has invalid type %r' %\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fetches, contraction_fn)\u001b[0m\n\u001b[1;32m    272\u001b[0m         raise TypeError('Fetch argument %r has invalid type %r, '\n\u001b[1;32m    273\u001b[0m                         \u001b[0;34m'must be a string or Tensor. (%s)'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m                         % (fetch, type(fetch), str(e)))\n\u001b[0m\u001b[1;32m    275\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         raise ValueError('Fetch argument %r cannot be interpreted as a '\n",
      "\u001b[0;31mTypeError\u001b[0m: Fetch argument array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n       [ 0.,  0.,  0., ...,  0.,  1.,  0.],\n       ..., \n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32) has invalid type <class 'numpy.ndarray'>, must be a string or Tensor. (Can not convert a ndarray into a Tensor or Operation.)"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    print(\"Restoring and Intializing\")\n",
    "    saver.restore(sess, \"./model_checks/model.ckpt\")\n",
    "#     print(\"h1 : %s\" % weights['h1'].eval())\n",
    "\n",
    "    # Run the initializer\n",
    "#     sess.run(init_global)\n",
    "    sess.run(init_local)\n",
    "#     print(\"h1 : %s\" % weights['h1'].eval())\n",
    "\n",
    "\n",
    "    # Tensorboard writers\n",
    "#     train_writer = tf.summary.FileWriter(\"./log_tb\" + '/train', sess.graph)\n",
    "#     test_writer = tf.summary.FileWriter(\"./log_tb\" + '/test')\n",
    "\n",
    "    print(\"Begin datetime: \" + str(datetime.datetime.now()))\n",
    "    # TRAINING ///\n",
    "    for step in range(1, num_steps+1):\n",
    "        try:\n",
    "            # Get batch and use begin/end indices to traverse dataset\n",
    "            batch_x, batch_y = sess.run(get_batch_train(train_ptr, train_ptr+batch_size))\n",
    "            train_ptr+=batch_size\n",
    "            \n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        except IndexError:\n",
    "            print(\"Finished an epoch, shuffling dataset\")\n",
    "            shuffle(train)\n",
    "            train_ptr=0\n",
    "\n",
    "        # VALIDATING and printing stats\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            # Calculate batch loss and accuracy\n",
    "#             summary, loss, mean_abs_train = sess.run([merged, loss_op, mae], feed_dict={X: batch_x, Y: batch_y})\n",
    "            loss, mean_abs_train = sess.run([loss_op, mae], feed_dict={X: batch_x, Y: batch_y})\n",
    "\n",
    "#             train_writer.add_summary(summary, tf.train.global_step(sess, global_step))\n",
    "\n",
    "            # Validation accuracy\n",
    "            try:\n",
    "                val_batch_x, val_batch_y = sess.run(get_batch_val(val_ptr, val_ptr+batch_size))\n",
    "                val_ptr+=batch_size\n",
    "            except StopIteration:\n",
    "                print(\"Reshuffling validation set\")\n",
    "                shuffle(val)\n",
    "                val_ptr=0\n",
    "\n",
    "                val_batch_x, val_batch_y = sess.run(get_batch_val(val_ptr, val_ptr+batch_size))\n",
    "                val_ptr+=batch_size\n",
    "\n",
    "#             summary, mean_abs_val = sess.run([merged, mae], feed_dict={X: val_batch_x, Y: val_batch_y})\n",
    "            mean_abs_val = sess.run([mae], feed_dict={X: val_batch_x, Y: val_batch_y})\n",
    "\n",
    "#             test_writer.add_summary(summary, tf.train.global_step(sess, global_step))\n",
    "\n",
    "            print(\"Step \" + str(step) + \"/\" + str(tf.train.global_step(sess, global_step)) + \", Minibatch Loss = \" + \\\n",
    "                  str(loss) + \", Training MAE = \" + \\\n",
    "                  str(mean_abs_train) + \", Eval MAE = \" + str(mean_abs_val))\n",
    "\n",
    "\n",
    "    print(\"End datetime: \" + str(datetime.datetime.now()))\n",
    "    # Save model\n",
    "#     print(\"h1 : %s\" % weights['h1'].eval())\n",
    "    save_path = saver.save(sess, \"./model_checks/model.ckpt\")\n",
    "    print(\"Model saved at: \" + str(save_path))\n",
    "\n",
    "    # Predict values for submission\n",
    "#     submission = np.array([[0,0]])\n",
    "#     counter = 0\n",
    "#     test_ptr = 0\n",
    "    \n",
    "#     while(True):\n",
    "#         if (test_ptr > num_samples_test):\n",
    "#             np.savetxt(\"./submission.txt\", submission, delimiter=',')\n",
    "#             print(\"Saved Submissions!\")\n",
    "#             break\n",
    "            \n",
    "#         try:\n",
    "#             if(counter % 10000 == 0):\n",
    "#                 print(\"batch # \" + str(counter))\n",
    "#             counter+=1\n",
    "\n",
    "#             test_batch = sess.run(get_batch_test(test_ptr, test_ptr+batch_size))\n",
    "#             test_ids = ids.iloc[test_ptr:test_ptr+batch_size].values.reshape((batch_size,-1))\n",
    "#             test_ptr+=batch_size\n",
    "\n",
    "#             predictions = sess.run([prediction], feed_dict={X: test_batch})\n",
    "#             preds = sess.run(tf.squeeze(preds)).reshape((batch_size, -1))\n",
    "#             new_subs = sess.run(tf.squeeze(np.array([test_ids, preds]).T))\n",
    "#             submission = np.concatenate((submission, new_subs))\n",
    "#             print(\"Concatenated!\")\n",
    "\n",
    "#         except IndexError:\n",
    "#             np.savetxt(\"./submission.txt\", submission, delimiter=',')\n",
    "#             print(\"Saved Submissions!\")\n",
    "#             break\n",
    "\n",
    "    print(\"Optimization Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
